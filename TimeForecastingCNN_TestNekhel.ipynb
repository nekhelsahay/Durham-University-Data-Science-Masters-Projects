{"cells":[{"cell_type":"markdown","metadata":{"id":"Eug0MzdClRqw"},"source":["#Earthquake time-forecasting CNN"]},{"cell_type":"markdown","metadata":{"id":"yj3Iat22lfjk"},"source":["**mount Google Drive as a disk to access files and data**\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8ibD96BjSKb"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRv_8gEDuHiN"},"outputs":[],"source":["cd \"/content/drive/MyDrive/Dissertation/Prototype 4\" #Change the directory name according to your convenience and where you have saved all the data files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gbn5nBMLfuHB"},"outputs":[],"source":["import pickle\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"iTpToRIhdOid"},"source":["**set choice for input files (determines the type of normalisation)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUEB1ixKdNJ4"},"outputs":[],"source":["choice = 'new2' # old=norm on all 3 components together. new2=no norm. new=norm on individual componenets, \n"]},{"cell_type":"markdown","metadata":{"id":"wxGJbhk4eCKE"},"source":["**set name of output model weights**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74Oj-_2xiJaS"},"outputs":[],"source":["# name of model weights file with version number of the training\n","tnum=1\n","WeightsFile='./model_'+str(tnum)+'.h5'\n","print('weights for the pending training will be saved in:',WeightsFile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"onFDQvl7PN_-"},"outputs":[],"source":["#WeightsFile='./model_0'+'.h5'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4dR8bN2lDSl"},"outputs":[],"source":["#WeightsFile='./bestweights'+'.h5'"]},{"cell_type":"markdown","metadata":{"id":"LrX74ryod6Qh"},"source":["**read input files**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhjKs-hgfHT9"},"outputs":[],"source":["normnoise = pickle.load( open( \"./Data/normnoise_\"+choice+\".p\", \"rb\" ) )\n","normpre = pickle.load( open( \"./Data/normpre_\"+choice+\".p\", \"rb\" ) )\n","testlist = pickle.load( open( \"./Data/testlist.p\", \"rb\" ) )\n","trainlist = pickle.load( open( \"./Data/trainlist.p\", \"rb\" ) )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7L5eu_hjmklZ"},"outputs":[],"source":["print(len(normpre))\n","print(len(trainlist))\n","print(len(testlist))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9P_qAMeiR69v"},"outputs":[],"source":["max(trainlist)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSMKJVEDfgQj"},"outputs":[],"source":["x_train = [normpre[i] for i in trainlist]\n","x_test = [normpre[i] for i in testlist]\n","x_noise_train = [normnoise[i] for i in trainlist]\n","x_noise_test = [normnoise[i] for i in testlist]\n","\n","window_length=16384\n","\n","#Make data into a format such that it can be concatenated\n","x_train2 = np.reshape(x_train, (len(x_train)*len(x_train[0]), window_length, 3))\n","x_test2 = np.reshape(x_test, (len(x_test)*len(x_test[0]), window_length, 3))\n","x_noise_train2 = np.expand_dims(x_noise_train, axis=-1)\n","x_noise_test2 = np.expand_dims(x_noise_test, axis=-1)\n","x_noise_train2 = np.reshape(x_noise_train2, (len(x_noise_train)*len(x_noise_train[0]), window_length, 3))\n","x_noise_test2 = np.reshape(x_noise_test2, (len(x_noise_test)*len(x_noise_test[0]), window_length, 3))\n","\n","# Generate ground truth (ones = 'precursor' class, zeros = 'noise' class)\n","y_train2 = np.ones(len(x_train2))\n","y_test2 =np.ones(len(x_test2))\n","y_noise_train2 = np.zeros(len(x_noise_train2))\n","y_noise_test2 = np.zeros(len(x_noise_test2))\n","y_train2  = y_train2.tolist()\n","y_test2 = y_test2.tolist()\n","y_noise_train2 = y_noise_train2.tolist()\n","y_noise_test2 = y_noise_test2.tolist()\n","\n","#Concatenate noise and precursor datasets \n","x_train = np.append(x_train2,x_noise_train2, axis=0)\n","x_test = np.append(x_test2,x_noise_test2, axis=0)\n","y_train = np.append(y_train2,y_noise_train2, axis=0)\n","y_test = np.append(y_test2,y_noise_test2, axis=0)"]},{"cell_type":"markdown","metadata":{"id":"ZdLWAvSur5ws"},"source":["**Build the CNN Footprint**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Az3n3oDOslbF"},"outputs":[],"source":["\n","!pip install pip install np_utils\n","\n","#from keras.utils import to_categorical\n","from keras.utils.np_utils import to_categorical\n","y_train= to_categorical(y_train, num_classes=2)\n","y_test= to_categorical(y_test, num_classes=2)\n","\n","import tensorflow as tf \n","import numpy as np\n","import time\n","import os\n","from keras.models import Model\n","from keras.layers import Conv1D, BatchNormalization, Add, MaxPooling1D, Dropout, Dense, Flatten, CuDNNLSTM, ZeroPadding1D\n","from keras.layers import Activation, Input, concatenate, GaussianNoise, GlobalMaxPooling1D, GlobalAveragePooling1D, Softmax, Permute, Multiply, Masking\n","#from keras.optimizers import Adam, RMSprop, SGD\n","from tensorflow.keras.optimizers import Adam,RMSprop, SGD\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras import backend as keras\n","from keras.regularizers import *  \n","\n","def encoder(input_shape, nb_classes):\n","    \n","    input_layer= Input(input_shape)\n","    random = Conv1D(3, 3, padding='same', kernel_initializer='random_normal')(input_layer)\n","\n","    X = Add()([input_layer, random])\n","\n","    conv3 = Conv1D(filters=32, kernel_size=7,padding='same', strides=1, kernel_initializer='random_normal')(X)\n","    conv3 = Conv1D(filters=32, kernel_size=7,padding='same', strides=1, kernel_initializer='random_normal')(conv3)\n","    conv3 = MaxPooling1D(3, strides=1)(conv3)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation('relu')(conv3)\n","\n","    conv4 = Conv1D(64, kernel_size=7,padding='same', strides=1, kernel_initializer='random_normal')(conv3)\n","    conv4 = Conv1D(64, kernel_size=7,padding='same', strides=1, kernel_initializer='random_normal')(conv4)        \n","    conv4 = MaxPooling1D(3, strides=2)(conv4)\n","    conv4 = BatchNormalization()(conv4)\n","    conv4 = Activation('relu')(conv4)\n","\n","    conv5 = Conv1D(filters=64, kernel_size=5, padding='same', dilation_rate=2, kernel_initializer='random_normal')(conv4)\n","    conv5 = MaxPooling1D(3, strides=1)(conv5)\n","    conv5 = BatchNormalization()(conv5)\n","    conv5 = Activation('relu')(conv5)\n","        \n","    conv3 = Conv1D(filters=128, kernel_size=5,padding='same', dilation_rate=4 , kernel_initializer='random_normal')(conv5)\n","    conv3 = MaxPooling1D(3, strides=1)(conv3)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation('relu')(conv3)\n","\n","    conv3 = Conv1D(filters=128, kernel_size=5,padding='same', dilation_rate=8 , kernel_initializer='random_normal')(conv3)\n","    conv3 = MaxPooling1D(3, strides=1)(conv3)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation('relu')(conv3)\n","\n","    conv4 = Conv1D(256, kernel_size=3,padding='same',  dilation_rate=16 , kernel_initializer='random_normal')(conv3)\n","    conv4 = MaxPooling1D(3, strides=1)(conv4)\n","    conv4 = BatchNormalization()(conv4)\n","    conv4 = Activation('relu')(conv4)\n","\n","    conv5 = Conv1D(filters=256, kernel_size=3, padding='same', dilation_rate=32, kernel_initializer='random_normal')(conv4)\n","    conv5 = MaxPooling1D(3, strides=1)(conv5)\n","    conv5 = BatchNormalization()(conv5)\n","    conv5 = Activation('relu')(conv5)\n","    conv5 = Dropout(0.02)(conv5)\n","  \n","    Y = GlobalAveragePooling1D()(conv5)\n","    Y = Dense(256, activation='relu', kernel_initializer='random_normal')(Y)\n","    Y = Dropout(0.02)(Y)\n","\n","  \n","    output_layer = Dense(nb_classes, activation='softmax', kernel_initializer='random_normal')(Y)\n","    \n","    model = Model(inputs=input_layer, outputs=output_layer)\n","\n","        \n","    return model      \n","    \n","model=encoder((window_length, 3), 2)\n","model.compile(loss='binary_crossentropy', optimizer=RMSprop(0.0001), metrics=['accuracy'])\n","model.summary()  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MTbUQMCQgWWE"},"outputs":[],"source":["from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n","from matplotlib import pyplot as plt\n","#from keras.callbacks import TensorBoard\n","#from keras import backend as K\n","\n","\n","class My_Callback(Callback):\n","\n","    def on_epoch_begin(self, epoch, logs={}):\n","        session = K.get_session()\n","        self.model.layers[1].kernel.initializer.run(session=session)\n","        return\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        #layer_index = 0  ## index of the layer you want to change\n","        # random weights to reset the layer\n","        #new_weights = np.random.randn(*self.model.layers[layer_index].get_weights().shape)\n","\n","        #self.model.layers[layer_index].set_weights(new_weights)\n","\n","        return\n","\n","callbacks = [\n","    EarlyStopping(patience=30, verbose=1),\n","    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.000001, verbose=1),\n","    ModelCheckpoint(filepath=WeightsFile, verbose=1, save_best_only=True, monitor='val_accuracy', save_weights_only=False, mode='max'),\n","    My_Callback()]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"znsQ0vmXf_te"},"source":["**Start training the CNN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tYxEjXiHgjrq"},"outputs":[],"source":["#!%%pypy\n","# This callback reinitialises the weights in the first layer of the neural network (random layer in my report) at the start of each epoch.\n","class My_Callback(Callback):\n","\n","    def on_epoch_begin(self, epoch, logs={}):\n","        # Code change to implement random layer in new version of TensorFlow\n","        # Get current weights\n","        weights = self.model.layers[1].get_weights()\n","        new_weights = []\n","        # Only the kernel is re-initialized, not the bais. For this reason I copy the old bais in the new weights.\n","        # I want the new weights to have the same distribution as the old ones. \n","        # For this reason, I scale the normal distribution by the stddev and mean of the initializer of the layer.\n","        # To make sure that the right number of weights are generated, I use the shape of the old weights as a template.\n","        new_weights.append(self.model.layers[1].kernel_initializer.stddev * np.random.randn(*weights[0].shape) + self.model.layers[1].kernel_initializer.mean)\n","        \n","        new_weights.append(weights[1])\n","        self.model.layers[1].set_weights(new_weights)\n","        \n","        return\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","\n","        return\n","\n","# Changed val_acc into val_accuracy in monitor to fir with new version of TensorFlow\n","\n","# Changed Early Stopping patience from 15 to 30 (old)\n","# Changed Early Stopping patience from 15 to 120 (new)\n","\n","callbacks = [\n","    EarlyStopping(patience=120, verbose=1),\n","    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n","    #ModelCheckpoint('weights_nonorm.h5', verbose=1, save_best_only=True, monitor='val_accuracy', save_weights_only=True),\n","    ModelCheckpoint(WeightsFile, verbose=1, save_best_only=True, monitor='val_accuracy', save_weights_only=True),\n","\n","My_Callback()]\n","\n","#model.fit(x_train, y_train, batch_size=16, epochs=100, shuffle=True, callbacks=callbacks,\n","#                    validation_data=(x_test, y_test))\n","history = model.fit(x_train, y_train, batch_size=32, epochs=100, shuffle=True, callbacks=callbacks, validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"qKOpR4ZekxO-"},"source":["**Loading the saved weights and testing the network on both train and test datasets:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLwyynh0qaSj"},"outputs":[],"source":["model.load_weights(WeightsFile)\n","Tr = model.evaluate(x_train, y_train, verbose=1)[-1]\n","Te = model.evaluate(x_test, y_test, verbose=1)[-1]\n","\n","print('Train Accuracy: ', Tr)\n","print('Test Accuracy: ', Te)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC8QE-eZXpy1"},"outputs":[],"source":["#Training and validation loss\n","\n","loss_train = history.history['loss']\n","loss_val = history.history['val_loss']\n","epochs = range(1,101)\n","plt.plot(epochs, loss_train, 'g', label='Training loss')\n","plt.plot(epochs, loss_val, 'b', label='validation loss')\n","plt.title('Training and Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnjtJ6N5ZiLB"},"outputs":[],"source":["#Training and validation accuracy\n","\n","loss_train = history.history['accuracy']\n","loss_val = history.history['val_accuracy']\n","epochs = range(1,101)\n","plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n","plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n","plt.title('Training and Validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSvgiY3RnGeO"},"outputs":[],"source":["\"\"\" import torch \n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as trans\n","\n","dataloader = torch.utils.data.DataLoader(x_train, batch_size=32) \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uz7DfuCwXzkH"},"outputs":[],"source":["\"\"\" from sklearn.metrics import confusion_matrix\n","\n","# simply call the confusion_matrix function to build a confusion matrix\n","cm = confusion_matrix(x_train, train_preds.argmax(dim=1))\n","print(cm) \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HiwG7BpISUOk"},"outputs":[],"source":["\"\"\" #Build a Confusion Matrix\n","\n","\n","\n","# get all the predictions for the entire training set\n","import torch\n","def get_all_preds(model, loader):\n","    all_preds = torch.tensor([])\n","    #model.eval() # set model to evaluate mode\n","    \n","    #for images, labels in loader:\n","        #images = images.to(device)\n","        #labels = labels.to(device)\n","        #preds = model(images)\n","        #all_preds = torch.cat((all_preds, preds), dim=0)\n","    \n","    return all_preds \n","\n","with torch.no_grad(): # disable gradient computations \n","    train_preds = get_all_preds(model, dataloader)\n","    # print(train_preds.shape) # (6000,10) \n","    # print(train_preds.requires_grad) # False\n","\n","print('true labels: ', x_train.targets.numpy())\n","print('pred labels: ', y_train.argmax(dim=1).numpy()) \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BN5ExG4MRK3e"},"outputs":[],"source":["\"\"\" #Plotting the confusion matrix\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import itertools\n","\n","def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=False):\n","    \n","    arguments\n","    ---------\n","    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n","\n","    target_names: given classification classes such as [0, 1, 2]\n","                  the class names, for example: ['high', 'medium', 'low']\n","\n","    title:        the text to display at the top of the matrix\n","\n","    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n","                  see http://matplotlib.org/examples/color/colormaps_reference.html\n","\n","    normalize:    If False, plot the raw numbers\n","                  If True, plot the proportions\n","    \n","     \n","    if cmap is None:\n","        cmap = plt.get_cmap('Oranges')\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    \n","    if target_names is not None:\n","        tick_marks = np.arange(len(target_names))\n","        plt.xticks(tick_marks, target_names, rotation=45)\n","        plt.yticks(tick_marks, target_names)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","\n","    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        if normalize:\n","            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","        else:\n","            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","\n","    plt.tight_layout()\n","    plt.ylim(len(target_names)-0.5, -0.5)\n","    plt.ylabel('True labels')\n","    plt.xlabel('Predicted labels')\n","    plt.savefig(title + '.png', dpi=500, bbox_inches = 'tight')\n","    plt.show()\n","    \n","    \n","    \n","# a tuple for all the class names\n","target_names = ('Noise', 'Precursors')\n","plot_confusion_matrix(cm, target_names) \"\"\""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"TimeForecastingCNN_TestNekhel.ipynb","provenance":[{"file_id":"146RMF2l8ua2Ve-yPCJT-zA-V5QoTkZRu","timestamp":1634733195266}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}